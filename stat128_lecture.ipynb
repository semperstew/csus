{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = \"/home/astewart/.jupyter/nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "cm.update(\"rise\", {\"theme\": \"serif\",\n",
    "                   \"transition\": \"zoom\",\n",
    "                   \"start_slideshow_at\": \"selected\",\n",
    "                   \"width\": \"100%\",\n",
    "                   \"height\": \"100%\",\n",
    "            \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 200%\">\n",
    "    <center>Neural Networks and Deep Learning</center>\n",
    "</h1>\n",
    "<br>\n",
    "<br>\n",
    "<figure>\n",
    "    <center><img src=\"https://cdn-images-1.medium.com/max/800/1*dnvGC-PORSoCo7VXT3PV_A.png\" width='65%'></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 150%\">\n",
    "    <center>What is Deep Learning?</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Another name for multi-layered artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - First proposed in 1944 by Warren McCullough and Walter Pitts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Went in and out of fashion for ~70 years, enjoying a resurgence in the last decade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Loosely modeled after the human brain, composed of thousands or millions of densely-connected processing nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 150%\"><center>Why did deep learning take so long to go mainstream?</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h3>#1: data volume</h3>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/08/Why-Deep-Learning-1024x742.png\" style=\"width: 40%\"></img>\n",
    "        <figcaption><cite>credit: Andrew Ng</cite></figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>#1: data volume</h3>\n",
    "<br>\n",
    "<blockquote>\n",
    "    \"Over 2.5 quintillion bytes of data are created every single day, and it’s only going to grow from there. By 2020, it’s estimated that 1.7MB of data will be created every second for every person on earth.\"\n",
    "    <footer><cite>Data Never Sleeps - 6.0</cite> Domo.com</footer>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 150%\"><center>Why did it take so long to go mainstream?</center></h1>\n",
    "\n",
    "<h3>#2: computational power</h3>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://cnet1.cbsistatic.com/img/lyL9qfCY61x88WWLApASPSJfh30=/2018/06/24/023f597e-7c5d-4dc1-b533-390bc9f28e17/supercomputer-power-increase.jpg\" style=\"width: 100%\"></img>\n",
    "        <figcaption><cite>Credit: CNet.com</cite></figcaption>\n",
    "    </center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 150%\"><center>Why did it take so long to go mainstream?</center></h1>\n",
    "\n",
    "<h3>#3: back-propagation algorithm</h3>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"http://neuralnetworksanddeeplearning.com/images/tikz21.png\" style=\"width: 50%\"></img>\n",
    "        <figcaption><cite>Credit: Michael Nielson</cite></figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - Backpropagation algorithm introduced by multiple researchers in the 60's.\n",
    " - Geoffrey Hinton co-authored first publication on the backpropagation algorithm for training multi-layer perceptron networks in 1986."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"150%\"><center>Logistic Regression and Perceptrons</center></h1>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://sebastianraschka.com/images/faq/logisticregr-neuralnet/schematic.png\" style=\"width: 50%\"></img>\n",
    "        <figcaption><cite>Credit: Sebastian Raschka</cite></figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - Logistic regression can be thought of as a single-layer neural network.\n",
    " - Today, one of the most common neuron activation functions is the sigmoid function.\n",
    " - Perceptron model is similar, minus the thresholding function. It applies weights to binary inputs, sums the weighted inputs and then uses some threshold to force a binary output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://sebastianraschka.com/images/faq/logisticregr-neuralnet/schematic.png\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>$$\\text{output}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & \\text{if} \\; \\sum_{j}w_{j}x_{j} \\le \\text{threshold} \\\\\n",
    "      1 & \\text{if} \\; \\sum_{j}w_{j}x_{j} > \\text{threshold} \\\\\n",
    "\\end{array} \n",
    "\\right. $$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2><center>Perceptron Model</center></h2>\n",
    "<br>\n",
    "<center>$$\\text{output}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & \\text{if} \\; \\sum_{j}w_{j}x_{j} \\le \\text{threshold} \\\\\n",
    "      1 & \\text{if} \\; \\sum_{j}w_{j}x_{j} > \\text{threshold} \\\\\n",
    "\\end{array} \n",
    "\\right. $$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>$$w \\cdot x \\equiv \\sum_{j}w_{j}x_{j}$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "$$b \\equiv -\\text{threshold}$$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>$$\\text{output}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & \\text{if} \\; w \\cdot x + b \\le 0 \\\\\n",
    "      1 & \\text{if} \\; w \\cdot x + b > 0 \\\\\n",
    "\\end{array} \n",
    "\\right. $$</center>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://www.intmath.com/laplace-transformation/svg/svgphp-unit-step-functions-definition-1a-s0.svg\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Sigmoid Model</center></h2>\n",
    "<br>\n",
    "<center>\n",
    "    $$\\sigma(z) \\equiv \\frac{1}{1+e^{-z}}$$\n",
    "</center>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - It looks like a smooth version of the step-function.\n",
    " - Let's replace z with our weighted sum from before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    $$\\sigma(z) \\equiv \\frac{1}{1+e^{-(w \\cdot x + b)}}$$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    $$\\Delta \\text{output} \\approx \\sum_j \\frac{\\partial \\, \\text{output}}{\\partial w_j}\\Delta w_j + \\frac{\\partial \\, \\text{output}}{\\partial b}\\Delta b $$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - As I said before, we essentially have a smoothed out version of the step-function and that makes all the difference in the world.\n",
    " - That means that small nudges to our weights and bias terms will cause small changes in our output.\n",
    " - A little Calculus can help us approximate that change.\n",
    " - Remember this little fact. We'll return to it in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Image Recognition</center></h1>\n",
    "\n",
    "// place example output here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://cdn-images-1.medium.com/max/616/1*Uhr-4VDJD0-gnteUNFzZTw.jpeg\" style=\"width: 60%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - This is only a small representation of a deep neural net.\n",
    " - We have a circle in the input layer for each pixel in our image.\n",
    " - As we move deeper in our network, each hidden layer is composed of the output of some special matrix operations (such as convolutions and pooling) which are fed to an activation function, like the sigmoid function.\n",
    " - The choice and ordering of these matrix operations are what really differentiate one Neural Network architecture from another.\n",
    " - Convolutions apply binary filters to the pixels and allow us to perform edge detection.\n",
    " - Pooling helps to control the number of parameters by taking the max or average of the values in the layers. This is important since more parameters in the model equates to longer computation time.\n",
    " - Hidden Layer 1 gives us patterns of local contrast such as edges. Essentially boundaries between dark and light pixels.\n",
    " - Hidden Layer 2 takes the outputs from Hidden Layer 1 and combines them together to form more abstract features like eyes, noses, lips and ears.\n",
    " - The final Hidden Layer takes the output from the previous layer, combining them together to form entire regions of a face.\n",
    " - Finally, the Output layer gives us a vectorized representation of the entire face. \n",
    " - Keep in mind, these are floating point numbers. So our network has learned to abstract the entire face and encode that knowledge into a vector of relatively few numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://slideplayer.com/slide/12295802/72/images/8/Feature+mapping+%F0%9D%91%A5+%3D+%5B+0.66%2C+0.3%2C+0.03+%E2%80%A6%5D+Raw+image+Feature+vector.jpg\" style=\"width: 70%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - one value for color, one for height, one for face shape, one for fluffiness, one for tail length, and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Example Input</center></h2>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure1.png\" style=\"width: 60%\"></img>\n",
    "    </center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - Color images have three channels Red-Green-Blue, each represented by a pixel matrix of intensity values.\n",
    " - The images I'm using are 200x200 and have an additional channel called a depth map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3><center>Apple FaceID Setup Process</center></h3>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://cdn-images-1.medium.com/max/800/1*wqhYOqBfBxiJpZMDBi19Rg.gif\" style=\"width: 80%\"></img>\n",
    "        <figcaption><cite>Credit: Apple Inc.</cite></figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3><center>Depth Map</center></h3>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://cdn-images-1.medium.com/max/800/1*kizoJc7jAowF7bh4MTxzDg.gif\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Example Convolutional Neural Network</center></h2>\n",
    "<br>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://www.researchgate.net/profile/Takio_Kurita/publication/320748406/figure/fig1/AS:555719381274624@1509505233044/An-example-of-CNN-architecture.png\" style=\"width: 70%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - Multiple convolution and pooling layers stacked on top of each other. \n",
    " - Again, convolutions allow us to detect features like edges\n",
    " - Pooling layers basically asks the regions of the input matrices whether a particular feature exists. It keeps only the \"best\" answer and throws away the other values. It doesn't need to know in which exact pixels the feature resides, only its location relative to the other features. As a helpful bonus, this also helps control the size of our network.\n",
    " - Generally, after each convolution-pooling pair, there will be some non-linear activation function (like the sigmoid). The output from this function will be the inputs to the next convolution-pooling pair.\n",
    " - Finally, we have the output layer. \n",
    " - So far, we've seen how a neural network feeds an input image forward through the layers to produce some abstract vector representation. But how does a neural network know if it is learning the correct representation of our input image???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The Cost Function</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Allows us to penalize the network for learning incorrect representation, and reward (penalize less) the network for learning a correct representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Ideally, we would also like to tell the network **how much** to change the output and in which direction, to become more correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"http://labs.centerforgov.org/Analytics-Training/images/warning.png\" style=\"width: 20%\"></img>\n",
    "    </center>\n",
    "</figure>\n",
    "<h2><center>Warning: Math Ahead!</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Quadratic Cost Function (a.k.a MSE)</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$C(w,b) \\equiv \\frac{1}{2n} \\sum_x \\|y(x)-a\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - $w$ is the collection of all weights in the network, $b$ is the collection of all biases in the network, $a$ is the vector of outputs from the network, $x$ is the vector of inputs, and $n$ is the number of samples.\n",
    " - Always non-negative since every term in the sum is non-negative.\n",
    " - $C(w,b) \\approx 0$ when $y(x) \\approx a$. Thus the network does a good job when it finds combinations of weights and biases such that $C(w,b) \\approx 0$.\n",
    " - That means minimizing $C(w,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://nikcheerla.github.io/deeplearningschool//media/loss3d.gif\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Gradient Descent</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 + \\frac{\\partial C}{\\partial v_2} \\Delta v_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - Let's keep things simple by replacing $w$ and $b$ with $v$, and considering only 2 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Delta v \\equiv (v_1,v_2)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- $\\Delta v$ is the vector of changes in $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\nabla C \\equiv \\left(\\frac{\\partial C}{\\partial v_1},\\frac{\\partial C}{\\partial v_2}\\right)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Gradient of $C$ is vector of partial derivatives with respect to $v$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Delta C \\approx \\nabla C \\cdot \\Delta v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Gradient helps us choose $\\Delta v$ so as to make $\\Delta C$ negative.\n",
    "- With a little foresight and some mathemagic, I have an idea what value to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2><center>Gradient Descent, cont.</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Delta v = -\\alpha \\nabla C$$\n",
    "$$\\alpha > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- $\\alpha$ is a very small positive number that we call the learning rate. It controls how far we move with each step. Choosing too large an $\\alpha$ and we could overshoot our minimum. Choosing too small of an $\\alpha$ and it'll take a long time to get there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Delta C \\approx -\\alpha \\nabla C \\cdot \\nabla C = -\\alpha \\|\\nabla C\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Since $\\|\\nabla C\\|^2 \\ge 0$, we can be sure that $\\Delta C \\le 0$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$v \\longrightarrow v' = v - \\alpha \\nabla C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>What was this for again?</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h5>Want to minimize cost function: $C(w,b) \\equiv \\frac{1}{2n} \\sum_x \\|y(x)-a\\|^2$</h5>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://nikcheerla.github.io/deeplearningschool//media/loss3d.gif\" style=\"width: 20%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w_k \\longrightarrow w_k' = w_k - \\alpha \\frac{\\partial C}{\\partial w_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$b_l \\longrightarrow b_l' = b_l - \\alpha \\frac{\\partial C}{\\partial b_l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- repreatedly apply this update until we reach the minimum. That is, there is nothing else that our network can learn from the data at hand to improve its estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://mommyvino.files.wordpress.com/2017/04/barack.jpeg?w=720\" style=\"width: 70%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Back to the fun stuff</center></h2>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://www.researchgate.net/profile/Takio_Kurita/publication/320748406/figure/fig1/AS:555719381274624@1509505233044/An-example-of-CNN-architecture.png\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"./images/sample.png\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"./images/sample2.png\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"./images/sample3.png\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Import libraries (lots of them)</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "import torchvision.utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageChops\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Define some helper functions</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img, text=None, should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis('off')\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic', fontweight='bold',\n",
    "                 bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 10})\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_plot(iteration, loss):\n",
    "    plt.plot(iteration, loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    train_dir = '/data/rgbd_face_data/kinect_cropped/train'\n",
    "    test_dir = '/data/rgbd_face_data/kinect_cropped/test'\n",
    "    train_batch_size = 32\n",
    "    train_number_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>The Data</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class FaceIdDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, imageFolderDataset, transform=None):\n",
    "        self.imageFolderDataset = imageFolderDataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        should_get_same_face = random.randint(0,1)\n",
    "        \n",
    "        if should_get_same_face:\n",
    "            while True:\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "                if img0_tuple[1] == img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "                if img0_tuple[1] != img1_tuple[1]:\n",
    "                    break\n",
    "        \n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "\n",
    "        depth0 = np.load(img0_tuple[0][:-5] + \"d.npy\")\n",
    "        depth1 = np.load(img1_tuple[0][:-5] + \"d.npy\")\n",
    "\n",
    "        rgbd0 = np.dstack((np.asarray(img0), depth0/255))\n",
    "        rgbd1 = np.dstack((np.asarray(img1), depth1/255))\n",
    "        \n",
    "        hog0 = np.load(img0_tuple[0][:-5] + \"h.npy\")\n",
    "        hog1 = np.load(img1_tuple[0][:-5] + \"h.npy\")\n",
    "        rgbd0 = np.dstack((rgbd0, hog0))\n",
    "        rgbd1 = np.dstack((rgbd1, hog1))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            rgbd0 = self.transform(rgbd0)\n",
    "            rgbd1 = self.transform(rgbd1)\n",
    "        \n",
    "        return rgbd0, rgbd1, torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_folders = dset.ImageFolder(root=Config.train_dir)\n",
    "\n",
    "train_dataset = FaceIdDataset(imageFolderDataset=train_folders,\n",
    "                              transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vis_dataloader = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1,\n",
    "                        batch_size=8)\n",
    "\n",
    "dataiter = iter(vis_dataloader)\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "\n",
    "concatenated = torch.cat((example_batch[0][:,:3,:,:],example_batch[1][:,:3,:,:]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(example_batch[2].numpy().reshape((1,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Neural Network Architecture</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(5, 10, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(10),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(10, 20, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(20),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(20, 20, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(20))\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(20*100*100, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    " \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(256, 128))\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://cdn-images-1.medium.com/max/1200/1*XzVUiq-3lYFtZEW3XfmKqg.jpeg\" style=\"width: 40%\"></img>\n",
    "    </center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Contrastive Loss</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, output0, output1, label):\n",
    "        l2_norm = F.pairwise_distance(output0, output1)\n",
    "        contrastive_loss = torch.mean((1-label) * torch.pow(l2_norm, 2) + \n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - l2_norm, min=0.0), 2))\n",
    "        \n",
    "        return contrastive_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$L(W,Y,\\vec{X_1},\\vec{X_2}) = (1-Y)\\frac{1}{2}(D_W)^2 + (Y)\\frac{1}{2}{max(0,m-D_W)}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Train it!!</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=True,\n",
    "                              num_workers=1,\n",
    "                              batch_size=Config.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nnet = SiameseNetwork().cuda()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(nnet.parameters(), lr = 0.0001)\n",
    "counter = []\n",
    "loss_history = []\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(0, Config.train_number_epochs):\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        img0, img1, label = data\n",
    "        img0 = img0.float()\n",
    "        img1 = img1.float()\n",
    "        img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output0, output1 = nnet(img0, img1)\n",
    "        contrastive_loss = criterion(output0, output1, label)\n",
    "        contrastive_loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch, contrastive_loss.item()))\n",
    "            iteration += 10\n",
    "            counter.append(iteration)\n",
    "            loss_history.append(contrastive_loss.item())\n",
    "show_plot(counter, loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Some simple testing</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = dset.ImageFolder(root=Config.test_dir)\n",
    "faceid_dataset = FaceIdDataset(imageFolderDataset=test_dataset,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_dataloader = DataLoader(faceid_dataset, num_workers=6, batch_size=1, shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "x0,_,_ = next(dataiter)\n",
    "\n",
    "for i in range(10):\n",
    "    _,x1,label2 = next(dataiter)\n",
    "    concatenated = torch.cat((x0[:,:3,:,:],x1[:,:3,:,:]),0)\n",
    "    \n",
    "    output0, output1 = nnet(Variable(x0.float()).cuda(), Variable(x1.float()).cuda())\n",
    "    l2_norm = F.pairwise_distance(output0, output1)\n",
    "    imshow(torchvision.utils.make_grid(concatenated), 'Dissimilarity: {:.2f}'.format(l2_norm.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2><center>Choosing a threshold value</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = dset.ImageFolder(root=Config.test_dir)\n",
    "faceid_dataset = FaceIdDataset(imageFolderDataset=test_dataset,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_dataloader = DataLoader(faceid_dataset, num_workers=6, batch_size=1, shuffle=False)\n",
    "dataiter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sensitivity = []\n",
    "specificity = []\n",
    "f1 = []\n",
    "accuracy = []\n",
    "y_true = np.array([])\n",
    "distances = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for x0, x1, label in dataiter:\n",
    "    y_true = np.append(y_true, label.item())\n",
    "    output0, output1 = nnet(Variable(x0.float()).cuda(), Variable(x1.float()).cuda())\n",
    "    l2_norm = F.pairwise_distance(output0, output1)\n",
    "    distances = np.append(distances, l2_norm.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.2,0.8,201)\n",
    "for thresh in thresholds:\n",
    "    y_pred = (distances > thresh).astype(float)\n",
    "    sensitivity.append(precision_score(y_true, y_pred, pos_label=0))\n",
    "    specificity.append(recall_score(y_true, y_pred, pos_label=1))\n",
    "    f1.append(f1_score(y_true, y_pred, pos_label=0))\n",
    "    accuracy.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, sensitivity, marker='', color='blue', linewidth=2, label='sensitivity')\n",
    "plt.plot(thresholds, specificity, marker='', color='green', linewidth=2, label='specificity')\n",
    "plt.plot(thresholds, f1, marker='', color='red', linewidth=2, label='F1')\n",
    "plt.plot(thresholds, accuracy, marker='', color='purple', linewidth=2, label='accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
